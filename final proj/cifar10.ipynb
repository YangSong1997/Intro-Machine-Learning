{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "\n",
    "<h1 style=\"text-align:center;\">CSCI 420/520: Introduction to Machine Learning final exam, Fall 2017</h1>\n",
    "<h1 style=\"text-align:center;\">Problem 1</h1>\n",
    "\n",
    "<h1 style=\"text-align:center;\">Due by 1700 on Tuesday, December 11, 2017</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Yang Song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CIFAR10 dataset\n",
    "\n",
    "For this problem you will use the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  This consists of 32 x 32 images of 10 different classes; there are 50,000 training cases and 10,000 test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What you are to do\n",
    "\n",
    "* Create a CNN that will perform image classification.\n",
    "    * Train it with the CIFAR10 training data and evaluate it with the CIFAR10 test data.\n",
    "    * Your model should have at least one convolutional layer (my best model had four, but I had success with fewer).\n",
    "    * If you perform a lot of tuning, you should only submit your best model.\n",
    "* Populate this Jupyter notebook with the code needed to create, train, and run your CNN.\n",
    "    * As part of your model evaluation, your notebook should compute and print the confusion matrix and f1-scores.\n",
    "* You should submit your CNN stored as the HDF5 file <code>problem1.hdf5</code> (see below).\n",
    "* Your model should achieve &ge; 50% accuracy on the test set (I reached 50% in six epochs with a very simple model, and 67% in 10 epochs with a deeper network).  If your accuracy is around 10%, which would be the expected result of random guessing, then something is seriously amiss.\n",
    "* High accuracy is not of paramount importance for this problem; the goal of this question is whether you understand how to build and train the network and can explain what your network does and its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions you are to answer about your model\n",
    "\n",
    "Include your answers in this cell.\n",
    "\n",
    "1.  Explain which each part of your model does.  E.g., if you include a convolutional layer or a dropout stage, explain what goes on in that stage, and the stage's purpose (e.g., downsampling). I used several convolutional layers. Two consisted of 32 filters, each 3 x 3, and two consisted of 64 filters, each 3 x 3. They apply convolution operations to the input and pass the results to the next layer. This is for dealing with data in smaller grids, allowing more efficiency, while adding more layers could improve accuracy. I also used several dropout stages, just to make sure the model is not overfitted.\n",
    "\n",
    "2.  Explain the number of model parameters returned by the <code>summary()</code> method of the <code>Sequential</code> class. Layer parameters are shown in output. These are all trainable parameters\n",
    "\n",
    "3.  How many epochs of training did you use? I used 6 epochs. \n",
    "\n",
    "4.  Which classes are most confused with one another?  Least confused?  Explain whether these results make sense to you. The most confused ones are: classes 1 and 9, classes 2 and 10. One of the least confused is between 5 and 10. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data and scaling the data\n",
    "\n",
    "You should use the Keras interface to this dataset.  The first time you read the training and test sets Keras will download the data.\n",
    "\n",
    "**Very important!  Use the following block of code for scaling and label encoding and no other!  Do not perform any feature extraction such as PCA, either.  This is how I will test your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read and scaled!\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# There are 10 classes of images.\n",
    "num_classes = 10\n",
    "\n",
    "# The data, split between train and test sets.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Scale the data.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test  /= 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train_enc = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_enc  = keras.utils.to_categorical(y_test,  num_classes)\n",
    "\n",
    "print(\"Data read and scaled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code follows&hellip;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing the model...done!\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers, losses, optimizers\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('constructing the model...', end='')\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print('done!')\n",
    "\n",
    "opt = optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the model parameters\n",
    "\n",
    "Assuming your model is named <code>model</code>&hellip;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "50000/50000 [==============================] - 283s 6ms/step - loss: 2.0002 - acc: 0.2670 - val_loss: 1.7406 - val_acc: 0.3873\n",
      "Epoch 2/6\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 1.6965 - acc: 0.3882 - val_loss: 1.5705 - val_acc: 0.4366\n",
      "Epoch 3/6\n",
      "50000/50000 [==============================] - 277s 6ms/step - loss: 1.5663 - acc: 0.4324 - val_loss: 1.4414 - val_acc: 0.4747\n",
      "Epoch 4/6\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 1.4734 - acc: 0.4681 - val_loss: 1.3672 - val_acc: 0.5083\n",
      "Epoch 5/6\n",
      "50000/50000 [==============================] - 279s 6ms/step - loss: 1.4089 - acc: 0.4928 - val_loss: 1.3181 - val_acc: 0.5311\n",
      "Epoch 6/6\n",
      "50000/50000 [==============================] - 294s 6ms/step - loss: 1.3577 - acc: 0.5131 - val_loss: 1.2582 - val_acc: 0.5498\n",
      "Done training the model!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "\n",
    "model.fit(x_train, y_train_enc,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test_enc))\n",
    "print(\"Done training the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def compute_metrics (classifier, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function computes and prints various performance measures for a classifier.\n",
    "    \"\"\"\n",
    "    # Use the classifier to make predictions for the test set.\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Choose the class with the highest estimated probability.\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "    # Compute the confusion matrix.\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm, '\\n')\n",
    "\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples in each class).\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    np.set_printoptions(precision=3, linewidth=132)\n",
    "    print('Normalized confusion matrix')\n",
    "    print(cm_normalized, '\\n')\n",
    "\n",
    "    # The confusion matrix as percentages.\n",
    "    cm_percentage = 100 * cm_normalized\n",
    "    print('Confusion matrix as percentages')\n",
    "    print(np.array2string(cm_percentage, formatter={'float_kind':lambda x: \"%6.2f\" % x}), '\\n')\n",
    "    \n",
    "    # Precision, recall, and f-score.\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[590  27  43  28  23  10  22  27 164  66]\n",
      " [ 34 650   5   9   2   2  28  25  65 180]\n",
      " [ 93  16 265  76 207  79 124  84  28  28]\n",
      " [ 28   9  45 331  81 180 162 106  19  39]\n",
      " [ 36   8  53  64 466  38 162 144  17  12]\n",
      " [ 13   6  62 178  63 450  72 129  17  10]\n",
      " [  8   4  28  54  73  16 745  47   5  20]\n",
      " [ 16   2  19  57  64  73  43 675  10  41]\n",
      " [118  58  11  20   6   8  11  19 688  61]\n",
      " [ 44 121   8  10   7   6  48  56  62 638]] \n",
      "\n",
      "Normalized confusion matrix\n",
      "[[ 0.59   0.027  0.043  0.028  0.023  0.01   0.022  0.027  0.164  0.066]\n",
      " [ 0.034  0.65   0.005  0.009  0.002  0.002  0.028  0.025  0.065  0.18 ]\n",
      " [ 0.093  0.016  0.265  0.076  0.207  0.079  0.124  0.084  0.028  0.028]\n",
      " [ 0.028  0.009  0.045  0.331  0.081  0.18   0.162  0.106  0.019  0.039]\n",
      " [ 0.036  0.008  0.053  0.064  0.466  0.038  0.162  0.144  0.017  0.012]\n",
      " [ 0.013  0.006  0.062  0.178  0.063  0.45   0.072  0.129  0.017  0.01 ]\n",
      " [ 0.008  0.004  0.028  0.054  0.073  0.016  0.745  0.047  0.005  0.02 ]\n",
      " [ 0.016  0.002  0.019  0.057  0.064  0.073  0.043  0.675  0.01   0.041]\n",
      " [ 0.118  0.058  0.011  0.02   0.006  0.008  0.011  0.019  0.688  0.061]\n",
      " [ 0.044  0.121  0.008  0.01   0.007  0.006  0.048  0.056  0.062  0.638]] \n",
      "\n",
      "Confusion matrix as percentages\n",
      "[[ 59.00   2.70   4.30   2.80   2.30   1.00   2.20   2.70  16.40   6.60]\n",
      " [  3.40  65.00   0.50   0.90   0.20   0.20   2.80   2.50   6.50  18.00]\n",
      " [  9.30   1.60  26.50   7.60  20.70   7.90  12.40   8.40   2.80   2.80]\n",
      " [  2.80   0.90   4.50  33.10   8.10  18.00  16.20  10.60   1.90   3.90]\n",
      " [  3.60   0.80   5.30   6.40  46.60   3.80  16.20  14.40   1.70   1.20]\n",
      " [  1.30   0.60   6.20  17.80   6.30  45.00   7.20  12.90   1.70   1.00]\n",
      " [  0.80   0.40   2.80   5.40   7.30   1.60  74.50   4.70   0.50   2.00]\n",
      " [  1.60   0.20   1.90   5.70   6.40   7.30   4.30  67.50   1.00   4.10]\n",
      " [ 11.80   5.80   1.10   2.00   0.60   0.80   1.10   1.90  68.80   6.10]\n",
      " [  4.40  12.10   0.80   1.00   0.70   0.60   4.80   5.60   6.20  63.80]] \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.602     0.590     0.596      1000\n",
      "          1      0.721     0.650     0.684      1000\n",
      "          2      0.492     0.265     0.344      1000\n",
      "          3      0.400     0.331     0.362      1000\n",
      "          4      0.470     0.466     0.468      1000\n",
      "          5      0.522     0.450     0.483      1000\n",
      "          6      0.526     0.745     0.616      1000\n",
      "          7      0.514     0.675     0.584      1000\n",
      "          8      0.640     0.688     0.663      1000\n",
      "          9      0.583     0.638     0.609      1000\n",
      "\n",
      "avg / total      0.547     0.550     0.541     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[590,  27,  43,  28,  23,  10,  22,  27, 164,  66],\n",
       "       [ 34, 650,   5,   9,   2,   2,  28,  25,  65, 180],\n",
       "       [ 93,  16, 265,  76, 207,  79, 124,  84,  28,  28],\n",
       "       [ 28,   9,  45, 331,  81, 180, 162, 106,  19,  39],\n",
       "       [ 36,   8,  53,  64, 466,  38, 162, 144,  17,  12],\n",
       "       [ 13,   6,  62, 178,  63, 450,  72, 129,  17,  10],\n",
       "       [  8,   4,  28,  54,  73,  16, 745,  47,   5,  20],\n",
       "       [ 16,   2,  19,  57,  64,  73,  43, 675,  10,  41],\n",
       "       [118,  58,  11,  20,   6,   8,  11,  19, 688,  61],\n",
       "       [ 44, 121,   8,  10,   7,   6,  48,  56,  62, 638]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(model, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:     1.25822776012\n",
      "Test accuracy: 0.5498\n",
      "Done evaluating the model!\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test_enc, verbose=0)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"Done evaluating the model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "Make you sure you get the filename right, as in the following block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model...done!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model (assuming the variable name is \"model\").\n",
    "print('Saving the model...', end='')\n",
    "model.save('problem1.hdf5')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
