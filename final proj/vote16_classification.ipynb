{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "\n",
    "<h1 style=\"text-align:center;\">CSCI 420: Introduction to Machine Learning final exam</h1>\n",
    "<h1 style=\"text-align:center;\">Problem 3</h1>\n",
    "<h1 style=\"text-align:center;\">Due by 1700 on Tuesday, December 12, 2017</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Yang Song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classifier for the 2016 presidential election\n",
    "\n",
    "In this problem you will look at county-level data for the 2016 presidential election (\"county\" is used loosely here, since Virginia has independent cities that are also included in the data).\n",
    "\n",
    "The goal is to build a classifier to determine whether a county voted for Clinton or for Trump.\n",
    "\n",
    "The demographic and geographic features for each county are [listed below](#Extract-the-features).\n",
    "\n",
    "An interesting feature of the election was the so-called [Clinton Archipelago](http://www.vividmaps.com/2016/12/trumpland-and-clinton-archipelago.html): Clinton carried relatively few counties compared to Trump (but they were counties with large populations):\n",
    "<pre>\n",
    "Class breakdown:  training set  test set  total\n",
    "       Clinton       330          158       488\n",
    "         Trump      1670          954      2624\n",
    "</pre>\n",
    "\n",
    "The simplest classifier we can build from the training data would be to say all counties vote Trump.  This classifier would be correct for  \n",
    "$$\n",
    "  1 - \\frac{330}{330 + 1670} = 83.5\\%\n",
    "$$\n",
    "of the training cases, and\n",
    "$$\n",
    "1 - \\frac{158}{158 + 954} = 85.8\\%\n",
    "$$\n",
    "of the test cases!  So whatever classifier we build must do substantially better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The training and test data are in [<code>train_vote16.csv</code>](http://www.cs.wm.edu/~rml/teaching/ml/final/train_vote16.csv) and [<code>test_vote16.csv</code>](http://www.cs.wm.edu/~rml/teaching/ml/final/test_vote16.csv), respectively.  These are CSV files.\n",
    "\n",
    "Column 0 is the class label:\n",
    "<pre>\n",
    "0: won by Clinton\n",
    "1: won by Trump\n",
    "</pre>\n",
    "\n",
    "The next two columns are the actual number of votes from 2016.  We won't use those &ndash; that would be cheating!\n",
    "\n",
    "The next three columns are:\n",
    "<pre>\n",
    "state_abbr:  the state's abbreviation\n",
    "county_name: the name of the county or city\n",
    "state_fips:  the Federal Information Processing Standards code for the state\n",
    "FIPS:        the Federal Information Processing Standards code for the county or city\n",
    "</pre>\n",
    "\n",
    "These are followed by information about the 2012 election, including the proportion voting for Obama and Romney as well as the vote totals themselves:\n",
    "<pre>\n",
    "total_votes_2012: total number of votes cast in the 2012 election\n",
    "votes_dem_2012:   number of votes cast for the Democratic candidate in 2012\n",
    "votes_rep_2012:   number of votes cast for the Republican candidate in 2012\n",
    "Obama:            percentage voting for Obama in 2012\n",
    "Romney:           percentage voting for Romney in 2012\n",
    "</pre>\n",
    "\n",
    "The remaining columns are demographic data about the county or city:\n",
    "<pre>\n",
    "PST045214: Population, 2014 estimate\n",
    "PST040210: Population, 2010 (April 1) estimates base\n",
    "PST120214: Population, percent change - April 1, 2010 to July 1, 2014\n",
    "POP010210: Population, 2010\n",
    "AGE135214: Persons under 5 years, percent, 2014\n",
    "AGE295214: Persons under 18 years, percent, 2014\n",
    "AGE775214: Persons 65 years and over, percent, 2014\n",
    "SEX255214: Female persons, percent, 2014\n",
    "RHI125214: White alone, percent, 2014\n",
    "RHI225214: Black or African American alone, percent, 2014\n",
    "RHI325214: American Indian and Alaska Native alone, percent, 2014\n",
    "RHI425214: Asian alone, percent, 2014\n",
    "RHI525214: Native Hawaiian and Other Pacific Islander alone, percent, 2014\n",
    "RHI625214: Two or More Races, percent, 2014\n",
    "RHI725214: Hispanic or Latino, percent, 2014\n",
    "RHI825214: White alone, not Hispanic or Latino, percent, 2014\n",
    "POP715213: Living in same house 1 year & over, percent, 2009-2013\n",
    "POP645213: Foreign born persons, percent, 2009-2013\n",
    "POP815213: Language other than English spoken at home, pct age 5+, 2009-2013\n",
    "EDU635213: High school graduate or higher, percent of persons age 25+, 2009-2013\n",
    "EDU685213: Bachelor's degree or higher, percent of persons age 25+, 2009-2013\n",
    "VET605213: Veterans, 2009-2013\n",
    "LFE305213: Mean travel time to work (minutes), workers age 16+, 2009-2013\n",
    "HSG010214: Housing units, 2014\n",
    "HSG445213: Homeownership rate, 2009-2013\n",
    "HSG096213: Housing units in multi-unit structures, percent, 2009-2013\n",
    "HSG495213: Median value of owner-occupied housing units, 2009-2013\n",
    "HSD410213: Households, 2009-2013\n",
    "HSD310213: Persons per household, 2009-2013\n",
    "INC910213: Per capita money income in past 12 months (2013 dollars), 2009-2013\n",
    "INC110213: Median household income, 2009-2013\n",
    "PVY020213: Persons below poverty level, percent, 2009-2013\n",
    "BZA010213: Private nonfarm establishments, 2013\n",
    "BZA110213: Private nonfarm employment,  2013\n",
    "BZA115213: Private nonfarm employment, percent change, 2012-2013\n",
    "NES010213: Nonemployer establishments, 2013\n",
    "SBO001207: Total number of firms, 2007\n",
    "SBO315207: Black-owned firms, percent, 2007\n",
    "SBO115207: American Indian- and Alaska Native-owned firms, percent, 2007\n",
    "SBO215207: Asian-owned firms, percent, 2007\n",
    "SBO515207: Native Hawaiian- and Other Pacific Islander-owned firms, percent, 2007\n",
    "SBO415207: Hispanic-owned firms, percent, 2007\n",
    "SBO015207: Women-owned firms, percent, 2007\n",
    "MAN450207: Manufacturers shipments, 2007 ($1,000)\n",
    "WTN220207: Merchant wholesaler sales, 2007 ($1,000)\n",
    "RTN130207: Retail sales, 2007 ($1,000)\n",
    "RTN131207: Retail sales per capita, 2007\n",
    "AFN120207: Accommodation and food services sales, 2007 ($1,000)\n",
    "BPS030214: Building permits, 2014\n",
    "LND110210: Land area in square miles, 2010\n",
    "POP060210: Population per square mile, 2010\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To Do\n",
    "\n",
    "* Build a classifier that predicts how a county votes with at least 97% overall accuracy.  It isn't too difficult to obtain 98% overall accuracy, and at least 95% accuracy for each candidate.  More accurate is better for this problem.\n",
    "* Populate this Jupyter notebook with the code needed to create, train, and run your CNN.\n",
    "    * As part of your model evaluation, your notebook should compute and print the confusion matrix and f1-scores.\n",
    "* You should build your classifier and any preprocessing as a scikit-learn <code>Pipeline</code>.\n",
    "* You should save your classification pipeline to the file <code>vote16_clf.pkl</code> using <code>sklearn.externals.joblib</code> (see below).\n",
    "* If you use a grid search or random search for hyperparameter tuning, you should indicate this in the write-up below, and submit the hyperparameters used in the submitted solution (you can just specify them in calls to the classifier's constructor).\n",
    "* If you try a classifier and it does not work to your liking, you should indicate this.  You can leave the code in the notebook, commented out, so I can see what you tried.\n",
    "\n",
    "* You can choose from any of the classifiers we studied, including\n",
    " - [k nearest neighbors](http://scikit-learn.org/stable/modules/neighbors.html),\n",
    " - [support vector machines / kernel machines](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html),\n",
    " - [decision trees](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier),\n",
    " - [logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html),\n",
    " - [neural networks](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) (you will need scikit-learn 0.18 for this; use <code>conda update scikit-learn</code>),\n",
    " - ensembles of the preceding (i.e., [random forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [AdaBoosted collections of weak learners](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)),\n",
    " - [neural networks](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    " \n",
    "## Note\n",
    "\n",
    "In order to build a pipeline you will need to implement your own first step of the pipeline in order to perform feature selection and any necessary feature encoding.  You can do so by creating your own class with a <code>fit()</code> and <code>transform()</code> method.  \n",
    "\n",
    "The calling sequences and return sequences of these two methods should be the same as those of the StandardScaler [<code>fit()</code>](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit) and [<code>transform()</code>](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.transform).\n",
    "\n",
    "For example, <code>fit()</code> might not do anything while <code>transform()</code> will return a subset of the variables as a numpy array.\n",
    "\n",
    "For full credit your pipeline should encapsulate all of your preprocessing steps.  If you cannot figure out how to make this work, you can enter your preprocessing steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to answer\n",
    "\n",
    "1. What classifiers did you try? I tried support vector machine, logistic regression, decision tree, and k-nearest-neighbors\n",
    "2. What tuning did you do? I used \"rbf\" kernel, which is considered good for complicated nonlinear data\n",
    "3. How did the classifiers perform? Except for the decision tree which only gave 0.924 accuracy, the other all acheived accuracies above 0.97, as commented in my code\n",
    "4. If you end up with a decision tree, explain how the tree makes its decisions.  For other classifiers, try to give some insight into why it works. I ended up using svm. It works because svm is very flexible on whether to do linear or nonlinear models, and rbf works well with data. It also does not have the knn shortcoming of being sensitive to outliers.\n",
    "5. What is cross-validation, and how can it be used in constructing classifiers? It means assessing how the results of a statistical analysis will generalize to an independent data set. It can be used as a k-fold cross-validation, where we randomly split the training dataset into k folds without replacement, in which k−1 folds are used for the model training and one fold is used for testing. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to submit\n",
    "\n",
    "1. This Jupyter notebook, filled out with code that when run will produce a working classifer with all necessary preprocessing.\n",
    "    * You should use your final hyperparameter values in the call to the classifier's constructor.\n",
    "2. If you get the pipeline working, the resulting <code>.pkl</code> file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code follows\n",
    "\n",
    "You are free to change the section titles, add sections, &amp;c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "print('reading data...', end='')\n",
    "df_train = pd.read_csv('train_vote16.csv')\n",
    "df_test  = pd.read_csv('test_vote16.csv')\n",
    "print('done!')\n",
    "\n",
    "x_train = df_train.iloc[:,3:]\n",
    "x_test  = df_test.iloc[:,3:]\n",
    "\n",
    "y_train = df_train.iloc[:,0]\n",
    "y_test  = df_test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_cols = [\"total_votes_2012\",\"votes_dem_2012\",\"votes_gop_2012\",\"Obama\",\"Romney\",\"PST045214\",\"PST040210\",\"PST120214\",\"POP010210\",\"AGE135214\",\"AGE295214\",\"AGE775214\",\"SEX255214\",\"RHI125214\",\"RHI225214\",\"RHI325214\",\"RHI425214\",\"RHI525214\",\"RHI625214\",\"RHI725214\",\"RHI825214\",\"POP715213\",\"POP645213\",\"POP815213\",\"EDU635213\",\"EDU685213\",\"VET605213\",\"LFE305213\",\"HSG010214\",\"HSG445213\",\"HSG096213\",\"HSG495213\",\"HSD410213\",\"HSD310213\",\"INC910213\",\"INC110213\",\"PVY020213\",\"BZA010213\",\"BZA110213\",\"BZA115213\",\"NES010213\",\"SBO001207\",\"SBO315207\",\"SBO115207\",\"SBO215207\",\"SBO515207\",\"SBO415207\",\"SBO015207\",\"MAN450207\",\"WTN220207\",\"RTN130207\",\"RTN131207\",\"AFN120207\",\"BPS030214\",\"LND110210\",\"POP060210\"]\n",
    "x_train_var = x_train.loc[:,x_cols].as_matrix().astype(float) # training X\n",
    "x_test_var = x_test.loc[:,x_cols].as_matrix().astype(float)  # testing X\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "\n",
    "svm = SVC(kernel='rbf', C=100.0, gamma=0.01,random_state=0)\n",
    "#Svc = svm.fit(x_train_fit,y_train)\n",
    "# 0.982\n",
    "\n",
    "#lr = LogisticRegression(C=100.0, random_state=0)\n",
    "#log_reg = lr.fit(x_train_fit, y_train)\n",
    "# 0.980\n",
    "\n",
    "#tree = DecisionTreeClassifier(criterion='entropy', max_depth=6, random_state=0)\n",
    "#dec_tree = tree.fit(x_train_fit, y_train)\n",
    "# 0.924\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "#KNN = knn.fit(x_train_fit, y_train)\n",
    "# 0.955\n",
    "\n",
    "pipe_lr = Pipeline([('scaler', scaler),\n",
    "            ('pca', pca),\n",
    "            ('svm', svm)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipe_lr.fit(x_train_var, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: %.3f' % pipe_lr.score(x_test_var, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Save the classification pipeline\n",
    "\n",
    "&hellip;assuming it is named <code>clf</code>&hellip;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vote16_clf.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "print('Saving the classifier...')\n",
    "joblib.dump(clf, 'vote16_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
