{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.wm.edu/~rml/images/wm_horizontal_single_line_full_color.png\">\n",
    "\n",
    "<h1 style=\"text-align:center;\">CSCI 420/520: Introduction to Machine Learning, Fall 2017</h1>\n",
    "<h1 style=\"text-align:center;\">CNNs in Keras for the MNIST data set</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit where credit is due\n",
    "\n",
    "This notebook is based on code taken from https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "* [Getting set up](#Getting-set-up)\n",
    "* [Prepare the data](#Prepare-the-data)\n",
    "* [Specify the CNN](#Specify-the-CNN)\n",
    "* [Examine the model](#Examine-the-model)\n",
    "    * [Understanding the number of model parameters](#Understanding-the-number-of-model-parameters)\n",
    "    * [The layers in the CNN](#The-layers-in-the-CNN)\n",
    "* [Training the CNN](#Training-the-CNN)\n",
    "    * [A single epoch](#A-single-epoch)\n",
    "    * [Why is the training loss higher than the testing loss?](#Why-is-the-training-loss-higher-than-the-testing-loss?)\n",
    "* [Looking at the softmax probability estimates](#Looking-at-the-softmax-probability-estimates)\n",
    "* [Evaluating the model](#Evaluating-the-model)\n",
    "* [More epochs](#More-epochs)\n",
    "* [Keras callbacks](#Keras-callbacks)\n",
    "* [Looking at the trained network](#Looking-at-the-trained-network)\n",
    "* [Saving the model](#Saving-the-model)\n",
    "* [Data augmentation](#Data-augmentation)\n",
    "* [A textbook neural network](#A-textbook-neural-network)\n",
    "    * [The model parameters](#The-model-parameters)\n",
    "    * [Training the network](#Training-the-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting set up\n",
    "\n",
    "You will need to do the following to get Keras and TensorFlow working.\n",
    "\n",
    "1. Revert to Python 3.5: <code>conda install python=3.5</code>\n",
    "2. Install TensorFlow 1.4: <code>pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py3-none-any.whl</code>\n",
    "3. Specify TensorFlow as the backend for Keras: in <code>~/.keras/keras.json</code> change <code>theano</code> to <code>tensorflow</code>\n",
    "4. Install Keras: <code>pip install keras</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "Once again we will use the MNIST handwritten digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_mnist (vile):\n",
    "    \"\"\"\n",
    "    This function reads the MNIST .npy files and returns the feature vectors and their associated\n",
    "    class labels, and a list of the class labels.\n",
    "    \"\"\"\n",
    "    Z = np.load(vile)\n",
    "    m, n = Z.shape\n",
    "    X = np.float32(Z[:, 0:n-1])\n",
    "    y = Z[:, n-1]\n",
    "    classes = np.unique(y)\n",
    "    return X, y, classes\n",
    "\n",
    "print('Reading the data...', end='')\n",
    "x_train, y_train, classes = read_mnist('mnist_train.npy')\n",
    "x_test,  y_test,  classes = read_mnist('mnist_test.npy')\n",
    "print('done!')\n",
    "\n",
    "print('Shape of x_train after prefrobincation:', x_train.shape)\n",
    "print('Number of training cases: {0:5d}'.format(x_train.shape[0]))\n",
    "print('Number of test cases:     {0:5d}'.format(x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data\n",
    "\n",
    "Since the gray scale values are 8-bit integers in the range 0 to 255, we'll just divide by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Scaling data to the range [0,1]...', end='')\n",
    "x_train /= 255\n",
    "x_test  /= 255\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the class labels\n",
    "\n",
    "We'll use one-hot encoding for the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import utils as kutils\n",
    "\n",
    "# The number of classes.\n",
    "num_classes = 10\n",
    "\n",
    "print('Encoding the class labels...', end='')\n",
    "y_train_enc = kutils.to_categorical(y_train, num_classes)\n",
    "y_test_enc  = kutils.to_categorical(y_test,  num_classes)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform some pre-flight checks\n",
    "\n",
    "Make sure we understand the organization of the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# The input image dimensions.\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# The number of classes.\n",
    "num_classes = 10\n",
    "\n",
    "# Check whether the channels (e.g., RGB) should come first or second and indicate this in\n",
    "# the tuple input_shape.  This isn't an issue for the grayscale MNIST data.\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test  = x_test.reshape(x_test.shape[0],   1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test  = x_test.reshape(x_test.shape[0],   img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print('Shape of x_train after prefrobincation:', x_train.shape)\n",
    "print('Number of training cases: {0:5d}'.format(x_train.shape[0]))\n",
    "print('Number of test cases:     {0:5d}'.format(x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the CNN\n",
    "\n",
    "The layers of the CNN are as follows:\n",
    "1. the input layer (not explicitly specified),\n",
    "2. a convolutional layer consisting of 32 filters, each 3 x 3,\n",
    "3. a convolutional layer consisting of 64 filters, each 3 x 3,\n",
    "4. a pooling layer to downsample the output of layer 2,\n",
    "5. a dropout layer which randomly selects nodes to turn off for dropout regularization,\n",
    "6. a flattening layer to turn the 2d image into a 1d vector,\n",
    "7. a standard layer with all-to-all connections,\n",
    "8. a second dropout layer, and finally\n",
    "9. a standard layer with all-to-all connections that produces as output softmax estimates of the class probabilities.\n",
    "\n",
    "In addition, in the call to the call to the model's [<code>compile</code>](https://keras.io/models/sequential) method we must also specify the [optimization algorithm](https://keras.io/optimizers) to be used in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import models, layers, losses, optimizers\n",
    "\n",
    "print('constructing the model...', end='')\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, kernel_size=(3, 3),\n",
    "          activation='relu',\n",
    "          input_shape=input_shape))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=losses.categorical_crossentropy,\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss function: cross-entropy\n",
    "\n",
    "The measure of goodness of the model (the **loss function**) is cross-entropy.\n",
    "\n",
    "If there are $T$ training cases and $K$ classes, the cross-entropy is\n",
    "$$\n",
    "-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{k=1}^{K} y_{k}^{(t)} \\log p_{k}^{(t)},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "y_{k}^{(t)} = \\left\\{\n",
    "    \\begin{array}{cl}\n",
    "        1 & \\mbox{if $k$ is the class of training case $t$}; \\\\\n",
    "        0 & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p_{k}^{(t)} = \\mbox{estimated probability that training case $t$ is in class $k$}.\n",
    "$$\n",
    "The goal is to **minimize** the cross-entropy.\n",
    "\n",
    "To understand the cross-entropy, let case $t$ belong to class $k$, so $y_{k}^{(t)} = 1$.\n",
    "\n",
    "Suppose the NN completely misclassifies case $t$: $p_{k}^{(t)} = 0$.  Then the cross-entropy contains the term\n",
    "$$\n",
    "1 \\log 0 = -\\infty,\n",
    "$$\n",
    "which makes the cross-entropy infinite (note the minus sign in the defintion above).  \n",
    "\n",
    "On the other hand, if the NN is entirely certain case $t$ is of class $k$, then $p_{k}^{(t)} = 1$, and the contribution to the cross-entropy is \n",
    "$$\n",
    "1 \\log 1 = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the model\n",
    "\n",
    "We can get a summary of the model using the <code>summary()</code> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the number of model parameters\n",
    "\n",
    "Here is how to interpret the number of parameters.\n",
    "- In the first layer, there are 32 units, each with its own 3x3 filter.  This makes 9 x 32 = 288 parameters.  In addition, each of the 32 units has a bias term, for a total of 288 + 32 = 320 model parameters.\n",
    "- The second convolutional layers takes the 32 convolutions from the first layer and feeds them to 64 units.  This means there are 32 x 64 filters, each 3x3, for a total of 32 x 64 x 9 = 18432.  There are also 64 bias terms for the second layer, so we end up with 18432 + 64 = 18496 model parameters\n",
    "- The dense layer is taking the 9216 outputs from the flattening layer and feeding them to a dense layer with 128 units.  This makes for 9216 x 128 = 1,179,648 model parameters.  Addding the 128 bias terms yields 1,179,776 model parameters.\n",
    "- Finally, in the last layer, we have 128 outputs being fed to 10 units, each with a bias term, leading to 10 x 128 + 10 = 1290 model parameters.\n",
    "\n",
    "This model has a large number of parameters to determine during the training phase.  The dropout regularization help address overfitting due to the large number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The layers in the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "  print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the CNN as a graph using the [Keras plot_model utility](https://keras.io/utils/#plot_model).\n",
    "\n",
    "This requires the following Python packages:\n",
    "1. pydot,\n",
    "2. graphviz.\n",
    "\n",
    "In addition, you will need to install the [GraphViz](http://graphviz.org) graph visualization package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "kutils.plot_model(model, to_file='model.png')\n",
    "\n",
    "Image('model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CNN\n",
    "\n",
    "With large training sets one frequently takes optimization steps based on an approximate direction of steepest descent computed using only a subset of the the data.  We refer to the size of the subset used as the **batch size**.\n",
    "\n",
    "An **epoch** is a pass through the entire data set in the process of training/optimization.\n",
    "\n",
    "In each epoch, the batches are chosen randomly, whence the name **stochastic gradient descent** (SGD).\n",
    "\n",
    "SGD allows us to try steps more quickly, and avoid the time and space requirements of processing the entire training set.  If the training data are reasonably uniform in their distribution, a subset of the data should give results similar to the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single epoch\n",
    "\n",
    "We will use a single epoch of training to start.  This is done with the method [<code>fit</code>](https://keras.io/models/sequential) in the <code>Sequential</code> model class.\n",
    "\n",
    "We also will use the test data as validation data.  This is useful to detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.fit(x_train, y_train_enc,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test_enc))\n",
    "\n",
    "score = model.evaluate(x_test, y_test_enc, verbose=0)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the training loss higher than the testing loss?\n",
    "\n",
    "Notice that the accuracy reported for the training set is **lower** than that reported for the test set, and the loss report for the training set is **higher** than that reported for the test set.  This seems backwards.\n",
    "\n",
    "Per the [Keras FAQ](https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss):\n",
    "\n",
    "<blockquote>\n",
    "<p>\n",
    "A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.\n",
    "</p>\n",
    "<p>\n",
    "Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n",
    "</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the softmax probability estimates\n",
    "\n",
    "In Keras it is possible to [examine the outputs of the layers in the network](https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer).\n",
    "\n",
    "Let's take a look at the softmax probabilities for the first 11 test cases, and compare with the actual labels.\n",
    "\n",
    "The softmax layer is the eighth and last layer, which is layer 7 since we are counting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_softmax_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                      [model.layers[7].output])\n",
    "\n",
    "num_cases = 11\n",
    "softmax = get_softmax_layer_output([x_test[0:num_cases,:], 0])[0]\n",
    "\n",
    "print('label  estimated probabilities by class')\n",
    "print('       ', end='')\n",
    "for c in range(0, num_classes):\n",
    "    print('{0:3d}  '.format(c), end='')\n",
    "print()\n",
    "for t in range(0, num_cases):\n",
    "    print('{0:<5d}  '.format(y_test[t]), end='')\n",
    "    for c in range(0, num_classes):\n",
    "        print('{0:.1f}  '.format(softmax[t][c]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = np.load('hillary.npy')\n",
    "H = H.reshape(H.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " get_softmax_layer_output([H, 0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "For this model the method <code>predict()</code> returns the softmax probability estimates for each image.  We will take as our classification the class with the maximum estimated probability.\n",
    "\n",
    "As we have in the past we will look at the confusion matrix, precision, and recall for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def compute_metrics (classifier, X_test, y_test, classes):\n",
    "    \"\"\"\n",
    "    This function computes and prints various performance measures for a classifier.\n",
    "    \"\"\"\n",
    "    # Use the classifier to make predictions for the test set.\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Choose the class with the highest estimated probability.\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    print('Classes:', classes, '\\n')\n",
    "\n",
    "    # Compute the confusion matrix.\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=classes)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(cm, '\\n')\n",
    "\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples in each class).\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    np.set_printoptions(precision=3, linewidth=132)\n",
    "    print('Normalized confusion matrix')\n",
    "    print(cm_normalized, '\\n')\n",
    "\n",
    "    # The confusion matrix as percentages.\n",
    "    cm_percentage = 100 * cm_normalized\n",
    "    print('Confusion matrix as percentages')\n",
    "    print(np.array2string(cm_percentage, formatter={'float_kind':lambda x: \"%6.2f\" % x}), '\\n')\n",
    "    \n",
    "    # Precision, recall, and f-score.\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    return cm\n",
    "\n",
    "compute_metrics(model, x_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More epochs \n",
    "\n",
    "Keras makes it easy to continue the training where we left off earlier.  It saves the information needed to continue training the neural network.\n",
    "\n",
    "When we specify <code>initial_epoch</code>, the value of <code>epochs</code> is the epoch at which training ends, rather than the number of epochs we train over.\n",
    "\n",
    "That is, if we set <code>initial_epoch=3</code> and <code>epochs=4</code>, then we will only optimize over a single epoch.\n",
    "\n",
    "This convention is useful if we want to keep track of how many epochs of training have been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 2\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          initial_epoch = 1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras callbacks \n",
    "\n",
    "Keras has a number of [callback](https://keras.io/callbacks) functions that can be used to specify actions to occur during training.\n",
    "\n",
    "Callbacks include\n",
    "* <code>ModelCheckpoint</code>, which saves the model after every epoch;\n",
    "* <code>EarlyStopping</code>, which stops the training when a specified metric of quality has stopped improving;\n",
    "* <code>ReduceLROnPlateau</code>, which reduces the learning rate when a specified metric of quality has stopped improving; and\n",
    "* <code>TerminateOnNaN</code>, which terminates the training when a NaN is computed as the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the trained network\n",
    "\n",
    "We can look at the weights for layer <code>k</code> using <code>model.layers[k].get_weights()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard\n",
    "\n",
    "Alternatively, we can use the <code>TensorBoard</code> callback to log information in a format that can be read by the [TensorBoard utility](https://www.tensorflow.org/get_started/summaries_and_tensorboard) in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Saving the model\n",
    "\n",
    "We can use the model's <code>save</code> method to save the model in [HDF5 format](https://support.hdfgroup.org/HDF5).\n",
    "\n",
    "Saving the model save's the layout and the weights in the neural network, and also the optimizer's state so that you can continue training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('saving the model...', end='')\n",
    "model.save('mnist_cnn.hdf5')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the saved model using <code>keras.models.load_model</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model2 = load_model('mnist_cnn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data augmentation\n",
    "\n",
    "**Data augmentation** refers to creating new training examples from our training data.  This is especially useful when dealing with image data.\n",
    "\n",
    "We can create new images by applying random horizontal and vertical shifts to existing images.  This is useful for the digit data since the digits might not be perfectly centered.\n",
    "\n",
    "Other possible transmogrifications include flipping the image horizontally or vertically, rotating the images, and applying various types of scaling.  These augmentations are not helpful for the digit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reconstruct the model and train it using data augmentation to increase the effective size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import models, layers, losses, optimizers\n",
    "\n",
    "print('reconstructing the model...', end='')\n",
    "\n",
    "aug = models.Sequential()\n",
    "aug.add(layers.Conv2D(32, kernel_size=(3, 3),\n",
    "           activation='relu',\n",
    "           input_shape=input_shape))\n",
    "aug.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "aug.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "aug.add(layers.Dropout(0.25))\n",
    "aug.add(layers.Flatten())\n",
    "aug.add(layers.Dense(128, activation='relu'))\n",
    "aug.add(layers.Dropout(0.5))\n",
    "aug.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "aug.compile(loss=losses.categorical_crossentropy,\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify the generator to produce new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "  width_shift_range=0.1,               # randomly shift images horizontally (fraction of total width)\n",
    "  height_shift_range=0.1,              # randomly shift images vertically (fraction of total height)\n",
    "                                       # The remaining options are here for illustrative purposes only.\n",
    "  featurewise_center=False,            # set input mean to 0 over the dataset\n",
    "  samplewise_center=False,             # set each sample mean to 0\n",
    "  featurewise_std_normalization=False, # divide inputs by std of the dataset\n",
    "  samplewise_std_normalization=False,  # divide each input by its std\n",
    "  zca_whitening=False,                 # apply ZCA whitening \n",
    "  rotation_range=0,                    # randomly rotate images in the range (in degrees, 0 to 180)\n",
    "  horizontal_flip=False,               # randomly flip images\n",
    "  vertical_flip=False)                 # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model2.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                          steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n",
    "                          epochs=epochs,\n",
    "                          validation_data=(x_test, y_test),\n",
    "                          workers=4)\n",
    "\n",
    "score = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A textbook neural network\n",
    "\n",
    "By a textbook neural network we mean a single layer of units with every input (pixel) connected to every unit.  \n",
    "\n",
    "The input to each unit is the inner product of the unit's weights with the pixel values plus a bias term.  In Keras we can accomplish this by using a convolution filter the size of the image, and only computing the convolution where the images are perfectly aligned. \n",
    "\n",
    "The activation function is the classical sigmoidal function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import models, layers, losses, optimizers\n",
    "\n",
    "print('constructing the model...', end='')\n",
    "\n",
    "textbook = models.Sequential()\n",
    "textbook.add(layers.Conv2D(64, kernel_size=(28,28), padding='valid', \n",
    "                           input_shape=input_shape, activation='sigmoid'))\n",
    "textbook.add(layers.Flatten())\n",
    "textbook.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "textbook.compile(loss=losses.categorical_crossentropy,\n",
    "              optimizer=optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model parameters\n",
    "\n",
    "Let's look at the number of model parameters in our neural network.\n",
    "\n",
    "This network is considerably simpler than the CNNs we looked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textbook.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Because this neural network has fewer parameters than the CNNs, each training epoch goes much faster.\n",
    "\n",
    "On the other hand, it takes many epochs before we see the accuracy we saw after a single epoch for the more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "textbook.fit(x_train, y_train_enc,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test_enc))\n",
    "\n",
    "score = textbook.evaluate(x_test, y_test_enc, verbose=0)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
